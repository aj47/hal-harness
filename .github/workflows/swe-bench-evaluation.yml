name: SWE-bench Evaluation

on:
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Run ID for evaluation'
        required: true
        default: 'swe-lite-20250809-161802'
      max_workers:
        description: 'Maximum number of parallel workers'
        required: false
        default: '4'
      timeout:
        description: 'Timeout per instance in seconds'
        required: false
        default: '1800'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 480  # 8 hours
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        lfs: true  # Enable Git LFS for large files
    
    - name: Set up Docker
      run: |
        sudo systemctl start docker
        sudo usermod -aG docker $USER
        # Verify Docker is working
        docker --version
        docker info
    
    - name: Free up disk space
      run: |
        # Remove unnecessary packages to free up space
        sudo apt-get remove -y '^dotnet-.*'
        sudo apt-get remove -y '^llvm-.*'
        sudo apt-get remove -y 'php.*'
        sudo apt-get autoremove -y
        sudo apt-get autoclean -y
        # Show available disk space
        df -h
    
    - name: Install Micromamba
      run: |
        # Download and install micromamba using the official installer
        curl -Ls https://micro.mamba.pm/install.sh | bash -s -- -b -p ~/micromamba
        # Add to PATH for this session
        export PATH="~/micromamba/bin:$PATH"
        # Initialize micromamba
        ~/micromamba/bin/micromamba shell init -s bash -p ~/micromamba
        # Verify installation
        ~/micromamba/bin/micromamba --version
    
    - name: Create evaluation environment
      run: |
        ~/micromamba/bin/micromamba create -y -n swebench_hal python=3.11
        ~/micromamba/bin/micromamba run -n swebench_hal pip install -e src/swebench
        # Verify installation
        ~/micromamba/bin/micromamba run -n swebench_hal python -c "import swebench; print('SWE-bench installed successfully')"
    
    - name: Verify predictions file exists
      run: |
        PREDICTIONS_PATH="results/${{ github.event.inputs.run_id }}/${{ github.event.inputs.run_id }}_SWE_BENCH_SUBMISSIONS.jsonl"
        if [ ! -f "$PREDICTIONS_PATH" ]; then
          echo "Error: Predictions file not found at $PREDICTIONS_PATH"
          echo "Available files in results directory:"
          find results/ -name "*.jsonl" -type f
          exit 1
        fi
        echo "Predictions file found: $PREDICTIONS_PATH"
        echo "File size: $(du -h "$PREDICTIONS_PATH" | cut -f1)"
        echo "Number of predictions: $(wc -l < "$PREDICTIONS_PATH")"
    
    - name: Run SWE-bench evaluation
      run: |
        PREDICTIONS_PATH="results/${{ github.event.inputs.run_id }}/${{ github.event.inputs.run_id }}_SWE_BENCH_SUBMISSIONS.jsonl"
        
        echo "Starting SWE-bench evaluation..."
        echo "Run ID: ${{ github.event.inputs.run_id }}"
        echo "Max workers: ${{ github.event.inputs.max_workers }}"
        echo "Timeout: ${{ github.event.inputs.timeout }}"
        echo "Predictions path: $PREDICTIONS_PATH"
        
        ~/micromamba/bin/micromamba run -n swebench_hal python -m swebench.harness.run_evaluation \
          --dataset_name princeton-nlp/SWE-bench_Lite \
          --predictions_path "$PREDICTIONS_PATH" \
          --max_workers ${{ github.event.inputs.max_workers }} \
          --run_id ${{ github.event.inputs.run_id }} \
          --timeout ${{ github.event.inputs.timeout }} \
          --verbose
    
    - name: Check evaluation results
      run: |
        echo "Evaluation completed. Checking results..."
        
        # Look for the main results file
        RESULTS_FILE="Auggie(sonnet4).${{ github.event.inputs.run_id }}.json"
        if [ -f "$RESULTS_FILE" ]; then
          echo "Results file found: $RESULTS_FILE"
          echo "Results summary:"
          cat "$RESULTS_FILE" | head -20
        else
          echo "Main results file not found. Looking for alternative result files..."
          find . -name "*.json" -path "./results/*" -o -name "*evaluation*" | head -10
        fi
        
        # Check logs directory
        if [ -d "logs" ]; then
          echo "Logs directory contents:"
          find logs/ -type f | head -10
        fi
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v4
      if: always()  # Upload even if evaluation fails
      with:
        name: swe-bench-evaluation-results-${{ github.event.inputs.run_id }}
        path: |
          Auggie(sonnet4).${{ github.event.inputs.run_id }}.json
          results/${{ github.event.inputs.run_id }}/evaluation.json
          logs/
        retention-days: 30
    
    - name: Upload evaluation logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: swe-bench-evaluation-logs-${{ github.event.inputs.run_id }}
        path: |
          logs/
        retention-days: 7
    
    - name: Display final summary
      if: always()
      run: |
        echo "=== SWE-bench Evaluation Summary ==="
        echo "Run ID: ${{ github.event.inputs.run_id }}"
        echo "Status: ${{ job.status }}"
        echo "Timestamp: $(date)"
        
        # Try to show results if available
        RESULTS_FILE="Auggie(sonnet4).${{ github.event.inputs.run_id }}.json"
        if [ -f "$RESULTS_FILE" ]; then
          echo "Final Results:"
          cat "$RESULTS_FILE"
        fi
        
        echo "Artifacts uploaded for download from the Actions tab."
